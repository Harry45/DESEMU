{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951d8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from numpyro.diagnostics import summary\n",
    "from utils.helpers import pickle_load\n",
    "import matplotlib.pylab as plt \n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font',**{'family':'sans-serif','serif':['Palatino']})\n",
    "figSize  = (12, 8)\n",
    "fontSize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7919161",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS = 'lsst'\n",
    "\n",
    "if ANALYSIS != 'lsst':\n",
    "    KEYS = ['sigma8', 'Omegac', 'Omegab', 'hubble', 'ns',\n",
    "            'm1', 'm2', 'm3', 'm4',\n",
    "            'dz_wl_1', 'dz_wl_2', 'dz_wl_3', 'dz_wl_4',\n",
    "            'a_ia', 'eta',\n",
    "            'b1', 'b2', 'b3', 'b4', 'b5', \n",
    "            'dz_gc_1', 'dz_gc_2', 'dz_gc_3', 'dz_gc_4', 'dz_gc_5']\n",
    "else:\n",
    "    KEYS = ['sigma8', 'omegac', 'omegab', 'hubble', 'ns', \"m1\", \"m2\", \"m3\", \"m4\", \"m5\",\n",
    "    \"dz_wl_1\", \"dz_wl_2\", \"dz_wl_3\", \"dz_wl_4\", \"dz_wl_5\",\n",
    "    \"a_ia\", \"eta\", \"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b7\", \"b8\", \"b9\", \"b10\",\n",
    "    \"dz_gc_1\", \"dz_gc_2\", \"dz_gc_3\", \"dz_gc_4\", \"dz_gc_5\",\n",
    "    \"dz_gc_6\", \"dz_gc_7\", \"dz_gc_8\", \"dz_gc_9\", \"dz_gc_10\"]\n",
    "\n",
    "# weight    \n",
    "# minuslogpost          \n",
    "# sigma8          \n",
    "# omegac          \n",
    "# omegab          \n",
    "# hubble              \n",
    "# ns              \n",
    "# m1     \n",
    "# m2              \n",
    "# m3              \n",
    "# m4              \n",
    "# m5         \n",
    "# dz_wl_1         \n",
    "# dz_wl_2         \n",
    "# dz_wl_3         \n",
    "# dz_wl_4         \n",
    "# dz_wl_5            \n",
    "# a_ia             \n",
    "# eta              \n",
    "# b1              \n",
    "# b2              \n",
    "# b3              \n",
    "# b4              \n",
    "# b5             \n",
    "# b6              \n",
    "# b7              \n",
    "# b8              \n",
    "# b9             \n",
    "# b10         \n",
    "# dz_gc_1         \n",
    "# dz_gc_2         \n",
    "# dz_gc_3         \n",
    "# dz_gc_4 \n",
    "# dz_gc_5         \n",
    "# dz_gc_6         \n",
    "# dz_gc_7         \n",
    "# dz_gc_8         \n",
    "# dz_gc_9        \n",
    "# dz_gc_10   \n",
    "# minuslogprior \n",
    "# minuslogprior__0    \n",
    "# chi2  \n",
    "# chi2__LSSTlike\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# weight    \n",
    "# minuslogpost          \n",
    "# sigma8          \n",
    "# omegac          \n",
    "# omegab          \n",
    "# hubble              \n",
    "# ns              \n",
    "# m1              \n",
    "# m2              \n",
    "# m3              \n",
    "# m4         \n",
    "# dz_wl_1         \n",
    "# dz_wl_2         \n",
    "# dz_wl_3         \n",
    "# dz_wl_4            \n",
    "# a_ia             \n",
    "# eta              \n",
    "# b1              \n",
    "# b2              \n",
    "# b3              \n",
    "# b4              \n",
    "# b5         \n",
    "# dz_gc_1         \n",
    "# dz_gc_2         \n",
    "# dz_gc_3         \n",
    "# dz_gc_4         \n",
    "# dz_gc_5   \n",
    "# minuslogprior \n",
    "# minuslogprior__0            \n",
    "# chi2 \n",
    "# chi2__my_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0197e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_calculation(samples1: np.ndarray, samples2: np.ndarray, neval: int) -> pd.DataFrame:\n",
    "    record = []\n",
    "    for i, key in enumerate(KEYS):\n",
    "        testsamples = np.vstack(([samples1[:,i], samples2[:,i]]))\n",
    "        summary_stats = summary(testsamples)\n",
    "        summary_stats[key] = summary_stats.pop('Param:0')\n",
    "        record.append(summary_stats)\n",
    "\n",
    "    record_df = []\n",
    "    for i in range(len(record)):\n",
    "        record_df.append(pd.DataFrame(record[i]).round(3).loc[['r_hat', 'n_eff', 'mean', 'std']])\n",
    "\n",
    "    record_df = pd.concat(record_df, axis = 1).T\n",
    "    record_df['scaled_n_eff'] = record_df['n_eff'] / neval\n",
    "    return record_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c874de8",
   "metadata": {},
   "source": [
    "## Cobaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62e2734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cobaya_statistics(engine = 'jaxcosmo'):\n",
    "    \n",
    "    record_samples = []\n",
    "    nsamples = []\n",
    "    nlike = 0\n",
    "    for i in range(2):\n",
    "        if ANALYSIS != 'lsst':\n",
    "            file = np.loadtxt(f'outputcobaya/testing/{engine}_{i+1}/output_prefix.1.txt')\n",
    "        else:\n",
    "            file = np.loadtxt(f'CobayaLSST/{engine}_{i+1}/lsst.1.txt')\n",
    "        weight = file[:,0]\n",
    "        samples = file[:,2:-4]\n",
    "        nlike += sum(weight)\n",
    "        record_samples.append(samples)\n",
    "        nsamples.append(samples.shape[0])\n",
    "\n",
    "    min_nsamples = min(nsamples)\n",
    "\n",
    "    stats = summary_calculation(record_samples[0][-min_nsamples:], record_samples[1][-min_nsamples:], nlike)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a252b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSST\n",
    "# 0.0002872206804902297 (Cobaya EH)\n",
    "# 0.026257363008476475 (NUTS EH - Glamdring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2db8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.41877584740861"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.026257363008476475 / 0.0002872206804902297"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce4e05",
   "metadata": {},
   "source": [
    "## EMCEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58b4baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emcee_stats(engine = 'jaxcosmo'):\n",
    "\n",
    "    test_1 = pickle_load('samples', f'{engine}_emcee_1')\n",
    "    test_2 = pickle_load('samples', f'{engine}_emcee_2')\n",
    "\n",
    "    nevals = test_1.flatchain.shape[0] + test_2.flatchain.shape[0]\n",
    "\n",
    "    samples_1 = test_1.flatchain #test_1.get_chain(discard = discard, thin = thin, flat = True) \n",
    "    samples_2 = test_2.flatchain #test_2.get_chain(discard = discard, thin = thin, flat = True)\n",
    "    \n",
    "    \n",
    "    stats = summary_calculation(samples_1, samples_2, nevals)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ee3dd",
   "metadata": {},
   "source": [
    "## NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5346718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuts_stats(engine = 'jaxcosmo'):\n",
    "    \n",
    "    sampler = pickle_load('lsst', f'nuts_sampler_{engine}')\n",
    "\n",
    "    nsamples = sampler.num_chains * sampler.num_samples\n",
    "    num_steps = sampler.get_extra_fields(group_by_chain=True)['num_steps'].sum(1).sum(0).item()\n",
    "    samples = sampler.get_samples(group_by_chain=True)\n",
    "    record = []\n",
    "    for key in KEYS:\n",
    "        parameter_samples = samples[key]\n",
    "        summary_stats = summary(parameter_samples)\n",
    "        summary_stats[key] = summary_stats.pop('Param:0')\n",
    "        record.append(summary_stats)\n",
    "\n",
    "    record_df = []\n",
    "    for i in range(len(record)):\n",
    "        record_df.append(pd.DataFrame(record[i]).round(3).loc[['r_hat', 'n_eff', 'mean', 'std']])\n",
    "\n",
    "    record_df = pd.concat(record_df, axis = 1).T\n",
    "    record_df['scaled_n_eff'] = record_df['n_eff'] / num_steps\n",
    "    return record_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "820edf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cobaya_jc = cobaya_statistics(engine = 'jaxcosmo')\n",
    "df_emcee_jc = emcee_stats(engine = 'jaxcosmo')\n",
    "df_nuts_jc = nuts_stats(engine = 'jaxcosmo')\n",
    "\n",
    "df_cobaya_emu = cobaya_statistics(engine = 'emulator')\n",
    "df_emcee_emu = emcee_stats(engine = 'emulator')\n",
    "df_nuts_emu = nuts_stats(engine = 'emulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df709c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03415634511934331"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nuts_emu['scaled_n_eff'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbbd27cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004761498346259732"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cobaya_emu['scaled_n_eff'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e10cd7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04482185394258569"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nuts_jc['scaled_n_eff'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d63ab2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004543626076053639"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cobaya_jc['scaled_n_eff'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab5269ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.173444709095387"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nuts_emu['scaled_n_eff'].mean() / df_cobaya_emu['scaled_n_eff'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d93e726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.864776104444681"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nuts_jc['scaled_n_eff'].mean() / df_cobaya_jc['scaled_n_eff'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb46cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
